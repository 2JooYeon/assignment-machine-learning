{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.image as img\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load point data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "filename    = 'assignment_06_data.csv'\n",
    "data_load   = np.loadtxt(filename, delimiter = ',')\n",
    "\n",
    "x   = data_load[0, :]\n",
    "y   = data_load[1, :]\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "plt.plot(x, y, '.', color = 'blue')\n",
    "plt.title('data points')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(x, y, theta0, theta1):\n",
    "\n",
    "    n = len(x)\n",
    "    diff_square_sum = 0\n",
    "    loss = 0\n",
    "    \n",
    "    for i in range(0, n):\n",
    "        diff_square_sum += (theta0 + theta1*x[i] - y[i])**2\n",
    "\n",
    "    loss = diff_square_sum / (2*n)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute the gradient for each model parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_theta0(x, y, theta0, theta1):\n",
    "\n",
    "    n = len(x)\n",
    "    diff_sum = 0\n",
    "    dL = 0\n",
    "    \n",
    "    for i in range(0, n):\n",
    "        diff_sum += (theta0 + theta1*x[i] - y[i])\n",
    "    \n",
    "    dL = diff_sum / n\n",
    "    \n",
    "    return dL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_theta1(x, y, theta0, theta1):\n",
    "\n",
    "    n = len(x)\n",
    "    diff_sum = 0\n",
    "    dL = 0\n",
    "    \n",
    "    for i in range(0, n):\n",
    "        diff_sum += (theta0 + theta1*x[1] - y[i])*x[i]\n",
    "    \n",
    "    dL = diff_sum /n\n",
    "\n",
    "    return dL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gradient descent for each model parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration =    0, loss = 76.13670\n",
      "iteration =    1, loss = 38.92372\n",
      "iteration =    2, loss = 16.69635\n",
      "iteration =    3, loss = 9.45456\n",
      "iteration =    4, loss = 17.19831\n",
      "iteration =    5, loss = 39.92756\n",
      "iteration =    6, loss = 77.64226\n",
      "iteration =    7, loss = 130.34239\n",
      "iteration =    8, loss = 198.02790\n",
      "iteration =    9, loss = 280.69875\n",
      "iteration =   10, loss = 378.35492\n",
      "iteration =   11, loss = 490.99637\n",
      "iteration =   12, loss = 618.62305\n",
      "iteration =   13, loss = 761.23495\n",
      "iteration =   14, loss = 918.83202\n",
      "iteration =   15, loss = 1091.41423\n",
      "iteration =   16, loss = 1278.98156\n",
      "iteration =   17, loss = 1481.53396\n",
      "iteration =   18, loss = 1699.07142\n",
      "iteration =   19, loss = 1931.59389\n",
      "iteration =   20, loss = 2179.10136\n",
      "iteration =   21, loss = 2441.59379\n",
      "iteration =   22, loss = 2719.07115\n",
      "iteration =   23, loss = 3011.53342\n",
      "iteration =   24, loss = 3318.98057\n",
      "iteration =   25, loss = 3641.41257\n",
      "iteration =   26, loss = 3978.82940\n",
      "iteration =   27, loss = 4331.23103\n",
      "iteration =   28, loss = 4698.61744\n",
      "iteration =   29, loss = 5080.98860\n",
      "iteration =   30, loss = 5478.34448\n",
      "iteration =   31, loss = 5890.68508\n",
      "iteration =   32, loss = 6318.01035\n",
      "iteration =   33, loss = 6760.32029\n",
      "iteration =   34, loss = 7217.61486\n",
      "iteration =   35, loss = 7689.89405\n",
      "iteration =   36, loss = 8177.15783\n",
      "iteration =   37, loss = 8679.40619\n",
      "iteration =   38, loss = 9196.63910\n",
      "iteration =   39, loss = 9728.85655\n",
      "iteration =   40, loss = 10276.05851\n",
      "iteration =   41, loss = 10838.24496\n",
      "iteration =   42, loss = 11415.41589\n",
      "iteration =   43, loss = 12007.57128\n",
      "iteration =   44, loss = 12614.71110\n",
      "iteration =   45, loss = 13236.83535\n",
      "iteration =   46, loss = 13873.94401\n",
      "iteration =   47, loss = 14526.03705\n",
      "iteration =   48, loss = 15193.11446\n",
      "iteration =   49, loss = 15875.17623\n",
      "iteration =   50, loss = 16572.22233\n",
      "iteration =   51, loss = 17284.25276\n",
      "iteration =   52, loss = 18011.26750\n",
      "iteration =   53, loss = 18753.26653\n",
      "iteration =   54, loss = 19510.24983\n",
      "iteration =   55, loss = 20282.21740\n",
      "iteration =   56, loss = 21069.16922\n",
      "iteration =   57, loss = 21871.10528\n",
      "iteration =   58, loss = 22688.02555\n",
      "iteration =   59, loss = 23519.93004\n",
      "iteration =   60, loss = 24366.81872\n",
      "iteration =   61, loss = 25228.69158\n",
      "iteration =   62, loss = 26105.54861\n",
      "iteration =   63, loss = 26997.38980\n",
      "iteration =   64, loss = 27904.21513\n",
      "iteration =   65, loss = 28826.02460\n",
      "iteration =   66, loss = 29762.81819\n",
      "iteration =   67, loss = 30714.59589\n",
      "iteration =   68, loss = 31681.35769\n",
      "iteration =   69, loss = 32663.10358\n",
      "iteration =   70, loss = 33659.83354\n",
      "iteration =   71, loss = 34671.54758\n",
      "iteration =   72, loss = 35698.24567\n",
      "iteration =   73, loss = 36739.92780\n",
      "iteration =   74, loss = 37796.59398\n",
      "iteration =   75, loss = 38868.24418\n",
      "iteration =   76, loss = 39954.87840\n",
      "iteration =   77, loss = 41056.49663\n",
      "iteration =   78, loss = 42173.09885\n",
      "iteration =   79, loss = 43304.68507\n",
      "iteration =   80, loss = 44451.25527\n",
      "iteration =   81, loss = 45612.80945\n",
      "iteration =   82, loss = 46789.34758\n",
      "iteration =   83, loss = 47980.86968\n",
      "iteration =   84, loss = 49187.37572\n",
      "iteration =   85, loss = 50408.86571\n",
      "iteration =   86, loss = 51645.33962\n",
      "iteration =   87, loss = 52896.79746\n",
      "iteration =   88, loss = 54163.23922\n",
      "iteration =   89, loss = 55444.66489\n",
      "iteration =   90, loss = 56741.07447\n",
      "iteration =   91, loss = 58052.46794\n",
      "iteration =   92, loss = 59378.84529\n",
      "iteration =   93, loss = 60720.20654\n",
      "iteration =   94, loss = 62076.55165\n",
      "iteration =   95, loss = 63447.88064\n",
      "iteration =   96, loss = 64834.19349\n",
      "iteration =   97, loss = 66235.49020\n",
      "iteration =   98, loss = 67651.77076\n",
      "iteration =   99, loss = 69083.03516\n",
      "iteration =  100, loss = 70529.28340\n",
      "iteration =  101, loss = 71990.51548\n",
      "iteration =  102, loss = 73466.73138\n",
      "iteration =  103, loss = 74957.93111\n",
      "iteration =  104, loss = 76464.11465\n",
      "iteration =  105, loss = 77985.28200\n",
      "iteration =  106, loss = 79521.43316\n",
      "iteration =  107, loss = 81072.56812\n",
      "iteration =  108, loss = 82638.68688\n",
      "iteration =  109, loss = 84219.78943\n",
      "iteration =  110, loss = 85815.87577\n",
      "iteration =  111, loss = 87426.94588\n",
      "iteration =  112, loss = 89052.99978\n",
      "iteration =  113, loss = 90694.03745\n",
      "iteration =  114, loss = 92350.05888\n",
      "iteration =  115, loss = 94021.06408\n",
      "iteration =  116, loss = 95707.05304\n",
      "iteration =  117, loss = 97408.02576\n",
      "iteration =  118, loss = 99123.98222\n",
      "iteration =  119, loss = 100854.92244\n",
      "iteration =  120, loss = 102600.84639\n",
      "iteration =  121, loss = 104361.75409\n",
      "iteration =  122, loss = 106137.64552\n",
      "iteration =  123, loss = 107928.52069\n",
      "iteration =  124, loss = 109734.37958\n",
      "iteration =  125, loss = 111555.22220\n",
      "iteration =  126, loss = 113391.04855\n",
      "iteration =  127, loss = 115241.85861\n",
      "iteration =  128, loss = 117107.65238\n",
      "iteration =  129, loss = 118988.42987\n",
      "iteration =  130, loss = 120884.19106\n",
      "iteration =  131, loss = 122794.93596\n",
      "iteration =  132, loss = 124720.66456\n",
      "iteration =  133, loss = 126661.37686\n",
      "iteration =  134, loss = 128617.07286\n",
      "iteration =  135, loss = 130587.75255\n",
      "iteration =  136, loss = 132573.41593\n",
      "iteration =  137, loss = 134574.06300\n",
      "iteration =  138, loss = 136589.69376\n",
      "iteration =  139, loss = 138620.30819\n",
      "iteration =  140, loss = 140665.90631\n",
      "iteration =  141, loss = 142726.48810\n",
      "iteration =  142, loss = 144802.05357\n",
      "iteration =  143, loss = 146892.60271\n",
      "iteration =  144, loss = 148998.13552\n",
      "iteration =  145, loss = 151118.65200\n",
      "iteration =  146, loss = 153254.15214\n",
      "iteration =  147, loss = 155404.63595\n",
      "iteration =  148, loss = 157570.10341\n",
      "iteration =  149, loss = 159750.55454\n",
      "iteration =  150, loss = 161945.98932\n",
      "iteration =  151, loss = 164156.40775\n",
      "iteration =  152, loss = 166381.80984\n",
      "iteration =  153, loss = 168622.19558\n",
      "iteration =  154, loss = 170877.56496\n",
      "iteration =  155, loss = 173147.91799\n",
      "iteration =  156, loss = 175433.25467\n",
      "iteration =  157, loss = 177733.57499\n",
      "iteration =  158, loss = 180048.87894\n",
      "iteration =  159, loss = 182379.16654\n",
      "iteration =  160, loss = 184724.43778\n",
      "iteration =  161, loss = 187084.69264\n",
      "iteration =  162, loss = 189459.93115\n",
      "iteration =  163, loss = 191850.15328\n",
      "iteration =  164, loss = 194255.35905\n",
      "iteration =  165, loss = 196675.54844\n",
      "iteration =  166, loss = 199110.72146\n",
      "iteration =  167, loss = 201560.87811\n",
      "iteration =  168, loss = 204026.01838\n",
      "iteration =  169, loss = 206506.14227\n",
      "iteration =  170, loss = 209001.24979\n",
      "iteration =  171, loss = 211511.34092\n",
      "iteration =  172, loss = 214036.41567\n",
      "iteration =  173, loss = 216576.47404\n",
      "iteration =  174, loss = 219131.51603\n",
      "iteration =  175, loss = 221701.54163\n",
      "iteration =  176, loss = 224286.55084\n",
      "iteration =  177, loss = 226886.54367\n",
      "iteration =  178, loss = 229501.52011\n",
      "iteration =  179, loss = 232131.48015\n",
      "iteration =  180, loss = 234776.42381\n",
      "iteration =  181, loss = 237436.35107\n",
      "iteration =  182, loss = 240111.26194\n",
      "iteration =  183, loss = 242801.15641\n",
      "iteration =  184, loss = 245506.03449\n",
      "iteration =  185, loss = 248225.89617\n",
      "iteration =  186, loss = 250960.74145\n",
      "iteration =  187, loss = 253710.57033\n",
      "iteration =  188, loss = 256475.38281\n",
      "iteration =  189, loss = 259255.17890\n",
      "iteration =  190, loss = 262049.95857\n",
      "iteration =  191, loss = 264859.72185\n",
      "iteration =  192, loss = 267684.46872\n",
      "iteration =  193, loss = 270524.19919\n",
      "iteration =  194, loss = 273378.91325\n",
      "iteration =  195, loss = 276248.61091\n",
      "iteration =  196, loss = 279133.29215\n",
      "iteration =  197, loss = 282032.95699\n",
      "iteration =  198, loss = 284947.60542\n",
      "iteration =  199, loss = 287877.23744\n",
      "iteration =  200, loss = 290821.85305\n",
      "iteration =  201, loss = 293781.45225\n",
      "iteration =  202, loss = 296756.03503\n",
      "iteration =  203, loss = 299745.60141\n",
      "iteration =  204, loss = 302750.15137\n",
      "iteration =  205, loss = 305769.68491\n",
      "iteration =  206, loss = 308804.20204\n",
      "iteration =  207, loss = 311853.70275\n",
      "iteration =  208, loss = 314918.18705\n",
      "iteration =  209, loss = 317997.65493\n",
      "iteration =  210, loss = 321092.10639\n",
      "iteration =  211, loss = 324201.54144\n",
      "iteration =  212, loss = 327325.96006\n",
      "iteration =  213, loss = 330465.36227\n",
      "iteration =  214, loss = 333619.74805\n",
      "iteration =  215, loss = 336789.11742\n",
      "iteration =  216, loss = 339973.47036\n",
      "iteration =  217, loss = 343172.80688\n",
      "iteration =  218, loss = 346387.12698\n",
      "iteration =  219, loss = 349616.43066\n",
      "iteration =  220, loss = 352860.71791\n",
      "iteration =  221, loss = 356119.98874\n",
      "iteration =  222, loss = 359394.24314\n",
      "iteration =  223, loss = 362683.48112\n",
      "iteration =  224, loss = 365987.70267\n",
      "iteration =  225, loss = 369306.90780\n",
      "iteration =  226, loss = 372641.09650\n",
      "iteration =  227, loss = 375990.26877\n",
      "iteration =  228, loss = 379354.42462\n",
      "iteration =  229, loss = 382733.56404\n",
      "iteration =  230, loss = 386127.68703\n",
      "iteration =  231, loss = 389536.79359\n",
      "iteration =  232, loss = 392960.88372\n",
      "iteration =  233, loss = 396399.95743\n",
      "iteration =  234, loss = 399854.01470\n",
      "iteration =  235, loss = 403323.05555\n",
      "iteration =  236, loss = 406807.07996\n",
      "iteration =  237, loss = 410306.08794\n",
      "iteration =  238, loss = 413820.07949\n",
      "iteration =  239, loss = 417349.05461\n",
      "iteration =  240, loss = 420893.01330\n",
      "iteration =  241, loss = 424451.95555\n",
      "iteration =  242, loss = 428025.88137\n",
      "iteration =  243, loss = 431614.79076\n",
      "iteration =  244, loss = 435218.68371\n",
      "iteration =  245, loss = 438837.56024\n",
      "iteration =  246, loss = 442471.42032\n",
      "iteration =  247, loss = 446120.26398\n",
      "iteration =  248, loss = 449784.09119\n",
      "iteration =  249, loss = 453462.90198\n",
      "iteration =  250, loss = 457156.69633\n",
      "iteration =  251, loss = 460865.47424\n",
      "iteration =  252, loss = 464589.23572\n",
      "iteration =  253, loss = 468327.98076\n",
      "iteration =  254, loss = 472081.70936\n",
      "iteration =  255, loss = 475850.42153\n",
      "iteration =  256, loss = 479634.11726\n",
      "iteration =  257, loss = 483432.79655\n",
      "iteration =  258, loss = 487246.45941\n",
      "iteration =  259, loss = 491075.10583\n",
      "iteration =  260, loss = 494918.73581\n",
      "iteration =  261, loss = 498777.34936\n",
      "iteration =  262, loss = 502650.94646\n",
      "iteration =  263, loss = 506539.52713\n",
      "iteration =  264, loss = 510443.09136\n",
      "iteration =  265, loss = 514361.63914\n",
      "iteration =  266, loss = 518295.17050\n",
      "iteration =  267, loss = 522243.68541\n",
      "iteration =  268, loss = 526207.18388\n",
      "iteration =  269, loss = 530185.66591\n",
      "iteration =  270, loss = 534179.13150\n",
      "iteration =  271, loss = 538187.58065\n",
      "iteration =  272, loss = 542211.01336\n",
      "iteration =  273, loss = 546249.42963\n",
      "iteration =  274, loss = 550302.82946\n",
      "iteration =  275, loss = 554371.21285\n",
      "iteration =  276, loss = 558454.57980\n",
      "iteration =  277, loss = 562552.93031\n",
      "iteration =  278, loss = 566666.26438\n",
      "iteration =  279, loss = 570794.58200\n",
      "iteration =  280, loss = 574937.88318\n",
      "iteration =  281, loss = 579096.16793\n",
      "iteration =  282, loss = 583269.43622\n",
      "iteration =  283, loss = 587457.68808\n",
      "iteration =  284, loss = 591660.92350\n",
      "iteration =  285, loss = 595879.14247\n",
      "iteration =  286, loss = 600112.34500\n",
      "iteration =  287, loss = 604360.53109\n",
      "iteration =  288, loss = 608623.70073\n",
      "iteration =  289, loss = 612901.85393\n",
      "iteration =  290, loss = 617194.99069\n",
      "iteration =  291, loss = 621503.11101\n",
      "iteration =  292, loss = 625826.21488\n",
      "iteration =  293, loss = 630164.30231\n",
      "iteration =  294, loss = 634517.37330\n",
      "iteration =  295, loss = 638885.42784\n",
      "iteration =  296, loss = 643268.46594\n",
      "iteration =  297, loss = 647666.48759\n",
      "iteration =  298, loss = 652079.49280\n",
      "iteration =  299, loss = 656507.48157\n",
      "iteration =  300, loss = 660950.45389\n",
      "iteration =  301, loss = 665408.40977\n",
      "iteration =  302, loss = 669881.34920\n",
      "iteration =  303, loss = 674369.27219\n",
      "iteration =  304, loss = 678872.17873\n",
      "iteration =  305, loss = 683390.06883\n",
      "iteration =  306, loss = 687922.94249\n",
      "iteration =  307, loss = 692470.79970\n",
      "iteration =  308, loss = 697033.64046\n",
      "iteration =  309, loss = 701611.46478\n",
      "iteration =  310, loss = 706204.27266\n",
      "iteration =  311, loss = 710812.06409\n",
      "iteration =  312, loss = 715434.83907\n",
      "iteration =  313, loss = 720072.59761\n",
      "iteration =  314, loss = 724725.33971\n",
      "iteration =  315, loss = 729393.06536\n",
      "iteration =  316, loss = 734075.77456\n",
      "iteration =  317, loss = 738773.46731\n",
      "iteration =  318, loss = 743486.14363\n",
      "iteration =  319, loss = 748213.80349\n",
      "iteration =  320, loss = 752956.44691\n",
      "iteration =  321, loss = 757714.07388\n",
      "iteration =  322, loss = 762486.68441\n",
      "iteration =  323, loss = 767274.27849\n",
      "iteration =  324, loss = 772076.85613\n",
      "iteration =  325, loss = 776894.41732\n",
      "iteration =  326, loss = 781726.96206\n",
      "iteration =  327, loss = 786574.49036\n",
      "iteration =  328, loss = 791437.00221\n",
      "iteration =  329, loss = 796314.49761\n",
      "iteration =  330, loss = 801206.97657\n",
      "iteration =  331, loss = 806114.43908\n",
      "iteration =  332, loss = 811036.88514\n",
      "iteration =  333, loss = 815974.31476\n",
      "iteration =  334, loss = 820926.72793\n",
      "iteration =  335, loss = 825894.12465\n",
      "iteration =  336, loss = 830876.50493\n",
      "iteration =  337, loss = 835873.86876\n",
      "iteration =  338, loss = 840886.21614\n",
      "iteration =  339, loss = 845913.54708\n",
      "iteration =  340, loss = 850955.86157\n",
      "iteration =  341, loss = 856013.15961\n",
      "iteration =  342, loss = 861085.44120\n",
      "iteration =  343, loss = 866172.70635\n",
      "iteration =  344, loss = 871274.95505\n",
      "iteration =  345, loss = 876392.18730\n",
      "iteration =  346, loss = 881524.40311\n",
      "iteration =  347, loss = 886671.60247\n",
      "iteration =  348, loss = 891833.78538\n",
      "iteration =  349, loss = 897010.95184\n",
      "iteration =  350, loss = 902203.10186\n",
      "iteration =  351, loss = 907410.23543\n",
      "iteration =  352, loss = 912632.35255\n",
      "iteration =  353, loss = 917869.45322\n",
      "iteration =  354, loss = 923121.53745\n",
      "iteration =  355, loss = 928388.60523\n",
      "iteration =  356, loss = 933670.65656\n",
      "iteration =  357, loss = 938967.69144\n",
      "iteration =  358, loss = 944279.70987\n",
      "iteration =  359, loss = 949606.71186\n",
      "iteration =  360, loss = 954948.69740\n",
      "iteration =  361, loss = 960305.66649\n",
      "iteration =  362, loss = 965677.61914\n",
      "iteration =  363, loss = 971064.55533\n",
      "iteration =  364, loss = 976466.47508\n",
      "iteration =  365, loss = 981883.37838\n",
      "iteration =  366, loss = 987315.26523\n",
      "iteration =  367, loss = 992762.13564\n",
      "iteration =  368, loss = 998223.98959\n",
      "iteration =  369, loss = 1003700.82710\n",
      "iteration =  370, loss = 1009192.64816\n",
      "iteration =  371, loss = 1014699.45277\n",
      "iteration =  372, loss = 1020221.24094\n",
      "iteration =  373, loss = 1025758.01265\n",
      "iteration =  374, loss = 1031309.76792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration =  375, loss = 1036876.50674\n",
      "iteration =  376, loss = 1042458.22911\n",
      "iteration =  377, loss = 1048054.93503\n",
      "iteration =  378, loss = 1053666.62451\n",
      "iteration =  379, loss = 1059293.29753\n",
      "iteration =  380, loss = 1064934.95411\n",
      "iteration =  381, loss = 1070591.59424\n",
      "iteration =  382, loss = 1076263.21792\n",
      "iteration =  383, loss = 1081949.82515\n",
      "iteration =  384, loss = 1087651.41594\n",
      "iteration =  385, loss = 1093367.99027\n",
      "iteration =  386, loss = 1099099.54816\n",
      "iteration =  387, loss = 1104846.08960\n",
      "iteration =  388, loss = 1110607.61459\n",
      "iteration =  389, loss = 1116384.12313\n",
      "iteration =  390, loss = 1122175.61523\n",
      "iteration =  391, loss = 1127982.09087\n",
      "iteration =  392, loss = 1133803.55007\n",
      "iteration =  393, loss = 1139639.99281\n",
      "iteration =  394, loss = 1145491.41911\n",
      "iteration =  395, loss = 1151357.82896\n",
      "iteration =  396, loss = 1157239.22237\n",
      "iteration =  397, loss = 1163135.59932\n",
      "iteration =  398, loss = 1169046.95982\n",
      "iteration =  399, loss = 1174973.30388\n",
      "iteration =  400, loss = 1180914.63149\n",
      "iteration =  401, loss = 1186870.94265\n",
      "iteration =  402, loss = 1192842.23736\n",
      "iteration =  403, loss = 1198828.51562\n",
      "iteration =  404, loss = 1204829.77743\n",
      "iteration =  405, loss = 1210846.02280\n",
      "iteration =  406, loss = 1216877.25171\n",
      "iteration =  407, loss = 1222923.46418\n",
      "iteration =  408, loss = 1228984.66020\n",
      "iteration =  409, loss = 1235060.83976\n",
      "iteration =  410, loss = 1241152.00288\n",
      "iteration =  411, loss = 1247258.14956\n",
      "iteration =  412, loss = 1253379.27978\n",
      "iteration =  413, loss = 1259515.39355\n",
      "iteration =  414, loss = 1265666.49088\n",
      "iteration =  415, loss = 1271832.57175\n",
      "iteration =  416, loss = 1278013.63618\n",
      "iteration =  417, loss = 1284209.68416\n",
      "iteration =  418, loss = 1290420.71569\n",
      "iteration =  419, loss = 1296646.73077\n",
      "iteration =  420, loss = 1302887.72940\n",
      "iteration =  421, loss = 1309143.71158\n",
      "iteration =  422, loss = 1315414.67732\n",
      "iteration =  423, loss = 1321700.62660\n",
      "iteration =  424, loss = 1328001.55944\n",
      "iteration =  425, loss = 1334317.47583\n",
      "iteration =  426, loss = 1340648.37577\n",
      "iteration =  427, loss = 1346994.25925\n",
      "iteration =  428, loss = 1353355.12629\n",
      "iteration =  429, loss = 1359730.97689\n",
      "iteration =  430, loss = 1366121.81103\n",
      "iteration =  431, loss = 1372527.62872\n",
      "iteration =  432, loss = 1378948.42997\n",
      "iteration =  433, loss = 1385384.21476\n",
      "iteration =  434, loss = 1391834.98311\n",
      "iteration =  435, loss = 1398300.73501\n",
      "iteration =  436, loss = 1404781.47045\n",
      "iteration =  437, loss = 1411277.18945\n",
      "iteration =  438, loss = 1417787.89200\n",
      "iteration =  439, loss = 1424313.57811\n",
      "iteration =  440, loss = 1430854.24776\n",
      "iteration =  441, loss = 1437409.90096\n",
      "iteration =  442, loss = 1443980.53771\n",
      "iteration =  443, loss = 1450566.15802\n",
      "iteration =  444, loss = 1457166.76188\n",
      "iteration =  445, loss = 1463782.34928\n",
      "iteration =  446, loss = 1470412.92024\n",
      "iteration =  447, loss = 1477058.47475\n",
      "iteration =  448, loss = 1483719.01281\n",
      "iteration =  449, loss = 1490394.53442\n",
      "iteration =  450, loss = 1497085.03958\n",
      "iteration =  451, loss = 1503790.52829\n",
      "iteration =  452, loss = 1510511.00056\n",
      "iteration =  453, loss = 1517246.45637\n",
      "iteration =  454, loss = 1523996.89574\n",
      "iteration =  455, loss = 1530762.31865\n",
      "iteration =  456, loss = 1537542.72512\n",
      "iteration =  457, loss = 1544338.11514\n",
      "iteration =  458, loss = 1551148.48871\n",
      "iteration =  459, loss = 1557973.84582\n",
      "iteration =  460, loss = 1564814.18649\n",
      "iteration =  461, loss = 1571669.51072\n",
      "iteration =  462, loss = 1578539.81849\n",
      "iteration =  463, loss = 1585425.10981\n",
      "iteration =  464, loss = 1592325.38468\n",
      "iteration =  465, loss = 1599240.64311\n",
      "iteration =  466, loss = 1606170.88508\n",
      "iteration =  467, loss = 1613116.11061\n",
      "iteration =  468, loss = 1620076.31969\n",
      "iteration =  469, loss = 1627051.51231\n",
      "iteration =  470, loss = 1634041.68849\n",
      "iteration =  471, loss = 1641046.84822\n",
      "iteration =  472, loss = 1648066.99150\n",
      "iteration =  473, loss = 1655102.11833\n",
      "iteration =  474, loss = 1662152.22872\n",
      "iteration =  475, loss = 1669217.32265\n",
      "iteration =  476, loss = 1676297.40013\n",
      "iteration =  477, loss = 1683392.46117\n",
      "iteration =  478, loss = 1690502.50575\n",
      "iteration =  479, loss = 1697627.53389\n",
      "iteration =  480, loss = 1704767.54557\n",
      "iteration =  481, loss = 1711922.54081\n",
      "iteration =  482, loss = 1719092.51960\n",
      "iteration =  483, loss = 1726277.48194\n",
      "iteration =  484, loss = 1733477.42783\n",
      "iteration =  485, loss = 1740692.35727\n",
      "iteration =  486, loss = 1747922.27026\n",
      "iteration =  487, loss = 1755167.16680\n",
      "iteration =  488, loss = 1762427.04689\n",
      "iteration =  489, loss = 1769701.91054\n",
      "iteration =  490, loss = 1776991.75773\n",
      "iteration =  491, loss = 1784296.58848\n",
      "iteration =  492, loss = 1791616.40277\n",
      "iteration =  493, loss = 1798951.20062\n",
      "iteration =  494, loss = 1806300.98202\n",
      "iteration =  495, loss = 1813665.74696\n",
      "iteration =  496, loss = 1821045.49546\n",
      "iteration =  497, loss = 1828440.22751\n",
      "iteration =  498, loss = 1835849.94311\n",
      "iteration =  499, loss = 1843274.64226\n",
      "iteration =  500, loss = 1850714.32496\n",
      "iteration =  501, loss = 1858168.99122\n",
      "iteration =  502, loss = 1865638.64102\n",
      "iteration =  503, loss = 1873123.27437\n",
      "iteration =  504, loss = 1880622.89128\n",
      "iteration =  505, loss = 1888137.49173\n",
      "iteration =  506, loss = 1895667.07574\n",
      "iteration =  507, loss = 1903211.64330\n",
      "iteration =  508, loss = 1910771.19440\n",
      "iteration =  509, loss = 1918345.72906\n",
      "iteration =  510, loss = 1925935.24727\n",
      "iteration =  511, loss = 1933539.74903\n",
      "iteration =  512, loss = 1941159.23434\n",
      "iteration =  513, loss = 1948793.70320\n",
      "iteration =  514, loss = 1956443.15562\n",
      "iteration =  515, loss = 1964107.59158\n",
      "iteration =  516, loss = 1971787.01109\n",
      "iteration =  517, loss = 1979481.41416\n",
      "iteration =  518, loss = 1987190.80077\n",
      "iteration =  519, loss = 1994915.17094\n",
      "iteration =  520, loss = 2002654.52465\n",
      "iteration =  521, loss = 2010408.86192\n",
      "iteration =  522, loss = 2018178.18274\n",
      "iteration =  523, loss = 2025962.48710\n",
      "iteration =  524, loss = 2033761.77502\n",
      "iteration =  525, loss = 2041576.04649\n",
      "iteration =  526, loss = 2049405.30151\n",
      "iteration =  527, loss = 2057249.54008\n",
      "iteration =  528, loss = 2065108.76221\n",
      "iteration =  529, loss = 2072982.96788\n",
      "iteration =  530, loss = 2080872.15710\n",
      "iteration =  531, loss = 2088776.32988\n",
      "iteration =  532, loss = 2096695.48620\n",
      "iteration =  533, loss = 2104629.62608\n",
      "iteration =  534, loss = 2112578.74950\n",
      "iteration =  535, loss = 2120542.85648\n",
      "iteration =  536, loss = 2128521.94701\n",
      "iteration =  537, loss = 2136516.02108\n",
      "iteration =  538, loss = 2144525.07871\n",
      "iteration =  539, loss = 2152549.11989\n",
      "iteration =  540, loss = 2160588.14462\n",
      "iteration =  541, loss = 2168642.15290\n",
      "iteration =  542, loss = 2176711.14473\n",
      "iteration =  543, loss = 2184795.12012\n",
      "iteration =  544, loss = 2192894.07905\n",
      "iteration =  545, loss = 2201008.02153\n",
      "iteration =  546, loss = 2209136.94757\n",
      "iteration =  547, loss = 2217280.85715\n",
      "iteration =  548, loss = 2225439.75029\n",
      "iteration =  549, loss = 2233613.62697\n",
      "iteration =  550, loss = 2241802.48721\n",
      "iteration =  551, loss = 2250006.33100\n",
      "iteration =  552, loss = 2258225.15834\n",
      "iteration =  553, loss = 2266458.96922\n",
      "iteration =  554, loss = 2274707.76366\n",
      "iteration =  555, loss = 2282971.54165\n",
      "iteration =  556, loss = 2291250.30320\n",
      "iteration =  557, loss = 2299544.04829\n",
      "iteration =  558, loss = 2307852.77693\n",
      "iteration =  559, loss = 2316176.48912\n",
      "iteration =  560, loss = 2324515.18487\n",
      "iteration =  561, loss = 2332868.86416\n",
      "iteration =  562, loss = 2341237.52701\n",
      "iteration =  563, loss = 2349621.17340\n",
      "iteration =  564, loss = 2358019.80335\n",
      "iteration =  565, loss = 2366433.41684\n",
      "iteration =  566, loss = 2374862.01389\n",
      "iteration =  567, loss = 2383305.59449\n",
      "iteration =  568, loss = 2391764.15864\n",
      "iteration =  569, loss = 2400237.70634\n",
      "iteration =  570, loss = 2408726.23759\n",
      "iteration =  571, loss = 2417229.75239\n",
      "iteration =  572, loss = 2425748.25074\n",
      "iteration =  573, loss = 2434281.73265\n",
      "iteration =  574, loss = 2442830.19810\n",
      "iteration =  575, loss = 2451393.64710\n",
      "iteration =  576, loss = 2459972.07966\n",
      "iteration =  577, loss = 2468565.49576\n",
      "iteration =  578, loss = 2477173.89542\n",
      "iteration =  579, loss = 2485797.27862\n",
      "iteration =  580, loss = 2494435.64538\n",
      "iteration =  581, loss = 2503088.99569\n",
      "iteration =  582, loss = 2511757.32955\n",
      "iteration =  583, loss = 2520440.64696\n",
      "iteration =  584, loss = 2529138.94792\n",
      "iteration =  585, loss = 2537852.23243\n",
      "iteration =  586, loss = 2546580.50049\n",
      "iteration =  587, loss = 2555323.75210\n",
      "iteration =  588, loss = 2564081.98726\n",
      "iteration =  589, loss = 2572855.20597\n",
      "iteration =  590, loss = 2581643.40824\n",
      "iteration =  591, loss = 2590446.59405\n",
      "iteration =  592, loss = 2599264.76342\n",
      "iteration =  593, loss = 2608097.91633\n",
      "iteration =  594, loss = 2616946.05280\n",
      "iteration =  595, loss = 2625809.17282\n",
      "iteration =  596, loss = 2634687.27638\n",
      "iteration =  597, loss = 2643580.36350\n",
      "iteration =  598, loss = 2652488.43417\n",
      "iteration =  599, loss = 2661411.48839\n",
      "iteration =  600, loss = 2670349.52616\n",
      "iteration =  601, loss = 2679302.54748\n",
      "iteration =  602, loss = 2688270.55235\n",
      "iteration =  603, loss = 2697253.54078\n",
      "iteration =  604, loss = 2706251.51275\n",
      "iteration =  605, loss = 2715264.46827\n",
      "iteration =  606, loss = 2724292.40735\n",
      "iteration =  607, loss = 2733335.32997\n",
      "iteration =  608, loss = 2742393.23615\n",
      "iteration =  609, loss = 2751466.12587\n",
      "iteration =  610, loss = 2760553.99915\n",
      "iteration =  611, loss = 2769656.85598\n",
      "iteration =  612, loss = 2778774.69635\n",
      "iteration =  613, loss = 2787907.52028\n",
      "iteration =  614, loss = 2797055.32776\n",
      "iteration =  615, loss = 2806218.11879\n",
      "iteration =  616, loss = 2815395.89337\n",
      "iteration =  617, loss = 2824588.65151\n",
      "iteration =  618, loss = 2833796.39319\n",
      "iteration =  619, loss = 2843019.11842\n",
      "iteration =  620, loss = 2852256.82720\n",
      "iteration =  621, loss = 2861509.51954\n",
      "iteration =  622, loss = 2870777.19542\n",
      "iteration =  623, loss = 2880059.85486\n",
      "iteration =  624, loss = 2889357.49784\n",
      "iteration =  625, loss = 2898670.12438\n",
      "iteration =  626, loss = 2907997.73447\n",
      "iteration =  627, loss = 2917340.32811\n",
      "iteration =  628, loss = 2926697.90529\n",
      "iteration =  629, loss = 2936070.46603\n",
      "iteration =  630, loss = 2945458.01032\n",
      "iteration =  631, loss = 2954860.53816\n",
      "iteration =  632, loss = 2964278.04956\n",
      "iteration =  633, loss = 2973710.54450\n",
      "iteration =  634, loss = 2983158.02299\n",
      "iteration =  635, loss = 2992620.48503\n",
      "iteration =  636, loss = 3002097.93063\n",
      "iteration =  637, loss = 3011590.35977\n",
      "iteration =  638, loss = 3021097.77247\n",
      "iteration =  639, loss = 3030620.16871\n",
      "iteration =  640, loss = 3040157.54851\n",
      "iteration =  641, loss = 3049709.91186\n",
      "iteration =  642, loss = 3059277.25875\n",
      "iteration =  643, loss = 3068859.58920\n",
      "iteration =  644, loss = 3078456.90320\n",
      "iteration =  645, loss = 3088069.20075\n",
      "iteration =  646, loss = 3097696.48185\n",
      "iteration =  647, loss = 3107338.74650\n",
      "iteration =  648, loss = 3116995.99470\n",
      "iteration =  649, loss = 3126668.22646\n",
      "iteration =  650, loss = 3136355.44176\n",
      "iteration =  651, loss = 3146057.64061\n",
      "iteration =  652, loss = 3155774.82302\n",
      "iteration =  653, loss = 3165506.98897\n",
      "iteration =  654, loss = 3175254.13848\n",
      "iteration =  655, loss = 3185016.27153\n",
      "iteration =  656, loss = 3194793.38814\n",
      "iteration =  657, loss = 3204585.48830\n",
      "iteration =  658, loss = 3214392.57201\n",
      "iteration =  659, loss = 3224214.63926\n",
      "iteration =  660, loss = 3234051.69007\n",
      "iteration =  661, loss = 3243903.72443\n",
      "iteration =  662, loss = 3253770.74234\n",
      "iteration =  663, loss = 3263652.74381\n",
      "iteration =  664, loss = 3273549.72882\n",
      "iteration =  665, loss = 3283461.69738\n",
      "iteration =  666, loss = 3293388.64949\n",
      "iteration =  667, loss = 3303330.58516\n",
      "iteration =  668, loss = 3313287.50437\n",
      "iteration =  669, loss = 3323259.40714\n",
      "iteration =  670, loss = 3333246.29345\n",
      "iteration =  671, loss = 3343248.16332\n",
      "iteration =  672, loss = 3353265.01674\n",
      "iteration =  673, loss = 3363296.85371\n",
      "iteration =  674, loss = 3373343.67422\n",
      "iteration =  675, loss = 3383405.47829\n",
      "iteration =  676, loss = 3393482.26591\n",
      "iteration =  677, loss = 3403574.03708\n",
      "iteration =  678, loss = 3413680.79180\n",
      "iteration =  679, loss = 3423802.53008\n",
      "iteration =  680, loss = 3433939.25190\n",
      "iteration =  681, loss = 3444090.95727\n",
      "iteration =  682, loss = 3454257.64620\n",
      "iteration =  683, loss = 3464439.31867\n",
      "iteration =  684, loss = 3474635.97470\n",
      "iteration =  685, loss = 3484847.61427\n",
      "iteration =  686, loss = 3495074.23740\n",
      "iteration =  687, loss = 3505315.84407\n",
      "iteration =  688, loss = 3515572.43430\n",
      "iteration =  689, loss = 3525844.00808\n",
      "iteration =  690, loss = 3536130.56541\n",
      "iteration =  691, loss = 3546432.10629\n",
      "iteration =  692, loss = 3556748.63072\n",
      "iteration =  693, loss = 3567080.13870\n",
      "iteration =  694, loss = 3577426.63023\n",
      "iteration =  695, loss = 3587788.10531\n",
      "iteration =  696, loss = 3598164.56395\n",
      "iteration =  697, loss = 3608556.00613\n",
      "iteration =  698, loss = 3618962.43186\n",
      "iteration =  699, loss = 3629383.84115\n",
      "iteration =  700, loss = 3639820.23398\n",
      "iteration =  701, loss = 3650271.61037\n",
      "iteration =  702, loss = 3660737.97031\n",
      "iteration =  703, loss = 3671219.31379\n",
      "iteration =  704, loss = 3681715.64083\n",
      "iteration =  705, loss = 3692226.95142\n",
      "iteration =  706, loss = 3702753.24556\n",
      "iteration =  707, loss = 3713294.52325\n",
      "iteration =  708, loss = 3723850.78449\n",
      "iteration =  709, loss = 3734422.02928\n",
      "iteration =  710, loss = 3745008.25762\n",
      "iteration =  711, loss = 3755609.46952\n",
      "iteration =  712, loss = 3766225.66496\n",
      "iteration =  713, loss = 3776856.84395\n",
      "iteration =  714, loss = 3787503.00650\n",
      "iteration =  715, loss = 3798164.15259\n",
      "iteration =  716, loss = 3808840.28224\n",
      "iteration =  717, loss = 3819531.39543\n",
      "iteration =  718, loss = 3830237.49218\n",
      "iteration =  719, loss = 3840958.57248\n",
      "iteration =  720, loss = 3851694.63633\n",
      "iteration =  721, loss = 3862445.68373\n",
      "iteration =  722, loss = 3873211.71468\n",
      "iteration =  723, loss = 3883992.72918\n",
      "iteration =  724, loss = 3894788.72723\n",
      "iteration =  725, loss = 3905599.70883\n",
      "iteration =  726, loss = 3916425.67398\n",
      "iteration =  727, loss = 3927266.62268\n",
      "iteration =  728, loss = 3938122.55494\n",
      "iteration =  729, loss = 3948993.47074\n",
      "iteration =  730, loss = 3959879.37010\n",
      "iteration =  731, loss = 3970780.25300\n",
      "iteration =  732, loss = 3981696.11946\n",
      "iteration =  733, loss = 3992626.96946\n",
      "iteration =  734, loss = 4003572.80302\n",
      "iteration =  735, loss = 4014533.62013\n",
      "iteration =  736, loss = 4025509.42079\n",
      "iteration =  737, loss = 4036500.20500\n",
      "iteration =  738, loss = 4047505.97276\n",
      "iteration =  739, loss = 4058526.72407\n",
      "iteration =  740, loss = 4069562.45893\n",
      "iteration =  741, loss = 4080613.17734\n",
      "iteration =  742, loss = 4091678.87930\n",
      "iteration =  743, loss = 4102759.56482\n",
      "iteration =  744, loss = 4113855.23388\n",
      "iteration =  745, loss = 4124965.88649\n",
      "iteration =  746, loss = 4136091.52266\n",
      "iteration =  747, loss = 4147232.14238\n",
      "iteration =  748, loss = 4158387.74564\n",
      "iteration =  749, loss = 4169558.33246\n",
      "iteration =  750, loss = 4180743.90283\n",
      "iteration =  751, loss = 4191944.45675\n",
      "iteration =  752, loss = 4203159.99421\n",
      "iteration =  753, loss = 4214390.51523\n",
      "iteration =  754, loss = 4225636.01980\n",
      "iteration =  755, loss = 4236896.50793\n",
      "iteration =  756, loss = 4248171.97960\n",
      "iteration =  757, loss = 4259462.43482\n",
      "iteration =  758, loss = 4270767.87359\n",
      "iteration =  759, loss = 4282088.29592\n",
      "iteration =  760, loss = 4293423.70179\n",
      "iteration =  761, loss = 4304774.09121\n",
      "iteration =  762, loss = 4316139.46419\n",
      "iteration =  763, loss = 4327519.82072\n",
      "iteration =  764, loss = 4338915.16079\n",
      "iteration =  765, loss = 4350325.48442\n",
      "iteration =  766, loss = 4361750.79160\n",
      "iteration =  767, loss = 4373191.08233\n",
      "iteration =  768, loss = 4384646.35661\n",
      "iteration =  769, loss = 4396116.61444\n",
      "iteration =  770, loss = 4407601.85582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration =  771, loss = 4419102.08075\n",
      "iteration =  772, loss = 4430617.28923\n",
      "iteration =  773, loss = 4442147.48126\n",
      "iteration =  774, loss = 4453692.65685\n",
      "iteration =  775, loss = 4465252.81598\n",
      "iteration =  776, loss = 4476827.95866\n",
      "iteration =  777, loss = 4488418.08490\n",
      "iteration =  778, loss = 4500023.19469\n",
      "iteration =  779, loss = 4511643.28802\n",
      "iteration =  780, loss = 4523278.36491\n",
      "iteration =  781, loss = 4534928.42535\n",
      "iteration =  782, loss = 4546593.46933\n",
      "iteration =  783, loss = 4558273.49687\n",
      "iteration =  784, loss = 4569968.50796\n",
      "iteration =  785, loss = 4581678.50260\n",
      "iteration =  786, loss = 4593403.48079\n",
      "iteration =  787, loss = 4605143.44254\n",
      "iteration =  788, loss = 4616898.38783\n",
      "iteration =  789, loss = 4628668.31667\n",
      "iteration =  790, loss = 4640453.22907\n",
      "iteration =  791, loss = 4652253.12501\n",
      "iteration =  792, loss = 4664068.00450\n",
      "iteration =  793, loss = 4675897.86755\n",
      "iteration =  794, loss = 4687742.71415\n",
      "iteration =  795, loss = 4699602.54429\n",
      "iteration =  796, loss = 4711477.35799\n",
      "iteration =  797, loss = 4723367.15524\n",
      "iteration =  798, loss = 4735271.93604\n",
      "iteration =  799, loss = 4747191.70039\n",
      "iteration =  800, loss = 4759126.44829\n",
      "iteration =  801, loss = 4771076.17974\n",
      "iteration =  802, loss = 4783040.89474\n",
      "iteration =  803, loss = 4795020.59329\n",
      "iteration =  804, loss = 4807015.27539\n",
      "iteration =  805, loss = 4819024.94105\n",
      "iteration =  806, loss = 4831049.59025\n",
      "iteration =  807, loss = 4843089.22300\n",
      "iteration =  808, loss = 4855143.83931\n",
      "iteration =  809, loss = 4867213.43917\n",
      "iteration =  810, loss = 4879298.02257\n",
      "iteration =  811, loss = 4891397.58953\n",
      "iteration =  812, loss = 4903512.14004\n",
      "iteration =  813, loss = 4915641.67410\n",
      "iteration =  814, loss = 4927786.19170\n",
      "iteration =  815, loss = 4939945.69286\n",
      "iteration =  816, loss = 4952120.17757\n",
      "iteration =  817, loss = 4964309.64584\n",
      "iteration =  818, loss = 4976514.09765\n",
      "iteration =  819, loss = 4988733.53301\n",
      "iteration =  820, loss = 5000967.95192\n",
      "iteration =  821, loss = 5013217.35439\n",
      "iteration =  822, loss = 5025481.74040\n",
      "iteration =  823, loss = 5037761.10997\n",
      "iteration =  824, loss = 5050055.46308\n",
      "iteration =  825, loss = 5062364.79975\n",
      "iteration =  826, loss = 5074689.11996\n",
      "iteration =  827, loss = 5087028.42373\n",
      "iteration =  828, loss = 5099382.71105\n",
      "iteration =  829, loss = 5111751.98192\n",
      "iteration =  830, loss = 5124136.23634\n",
      "iteration =  831, loss = 5136535.47431\n",
      "iteration =  832, loss = 5148949.69583\n",
      "iteration =  833, loss = 5161378.90090\n",
      "iteration =  834, loss = 5173823.08952\n",
      "iteration =  835, loss = 5186282.26169\n",
      "iteration =  836, loss = 5198756.41742\n",
      "iteration =  837, loss = 5211245.55669\n",
      "iteration =  838, loss = 5223749.67952\n",
      "iteration =  839, loss = 5236268.78589\n",
      "iteration =  840, loss = 5248802.87582\n",
      "iteration =  841, loss = 5261351.94929\n",
      "iteration =  842, loss = 5273916.00632\n",
      "iteration =  843, loss = 5286495.04690\n",
      "iteration =  844, loss = 5299089.07103\n",
      "iteration =  845, loss = 5311698.07871\n",
      "iteration =  846, loss = 5324322.06994\n",
      "iteration =  847, loss = 5336961.04472\n",
      "iteration =  848, loss = 5349615.00305\n",
      "iteration =  849, loss = 5362283.94493\n",
      "iteration =  850, loss = 5374967.87036\n",
      "iteration =  851, loss = 5387666.77934\n",
      "iteration =  852, loss = 5400380.67188\n",
      "iteration =  853, loss = 5413109.54796\n",
      "iteration =  854, loss = 5425853.40760\n",
      "iteration =  855, loss = 5438612.25078\n",
      "iteration =  856, loss = 5451386.07752\n",
      "iteration =  857, loss = 5464174.88781\n",
      "iteration =  858, loss = 5476978.68164\n",
      "iteration =  859, loss = 5489797.45903\n",
      "iteration =  860, loss = 5502631.21997\n",
      "iteration =  861, loss = 5515479.96446\n",
      "iteration =  862, loss = 5528343.69250\n",
      "iteration =  863, loss = 5541222.40409\n",
      "iteration =  864, loss = 5554116.09923\n",
      "iteration =  865, loss = 5567024.77792\n",
      "iteration =  866, loss = 5579948.44017\n",
      "iteration =  867, loss = 5592887.08596\n",
      "iteration =  868, loss = 5605840.71530\n",
      "iteration =  869, loss = 5618809.32820\n",
      "iteration =  870, loss = 5631792.92464\n",
      "iteration =  871, loss = 5644791.50464\n",
      "iteration =  872, loss = 5657805.06819\n",
      "iteration =  873, loss = 5670833.61528\n",
      "iteration =  874, loss = 5683877.14593\n",
      "iteration =  875, loss = 5696935.66013\n",
      "iteration =  876, loss = 5710009.15788\n",
      "iteration =  877, loss = 5723097.63918\n",
      "iteration =  878, loss = 5736201.10403\n",
      "iteration =  879, loss = 5749319.55243\n",
      "iteration =  880, loss = 5762452.98438\n",
      "iteration =  881, loss = 5775601.39988\n",
      "iteration =  882, loss = 5788764.79894\n",
      "iteration =  883, loss = 5801943.18154\n",
      "iteration =  884, loss = 5815136.54769\n",
      "iteration =  885, loss = 5828344.89740\n",
      "iteration =  886, loss = 5841568.23065\n",
      "iteration =  887, loss = 5854806.54746\n",
      "iteration =  888, loss = 5868059.84782\n",
      "iteration =  889, loss = 5881328.13173\n",
      "iteration =  890, loss = 5894611.39918\n",
      "iteration =  891, loss = 5907909.65019\n",
      "iteration =  892, loss = 5921222.88475\n",
      "iteration =  893, loss = 5934551.10286\n",
      "iteration =  894, loss = 5947894.30452\n",
      "iteration =  895, loss = 5961252.48973\n",
      "iteration =  896, loss = 5974625.65850\n",
      "iteration =  897, loss = 5988013.81081\n",
      "iteration =  898, loss = 6001416.94667\n",
      "iteration =  899, loss = 6014835.06609\n",
      "iteration =  900, loss = 6028268.16905\n",
      "iteration =  901, loss = 6041716.25557\n",
      "iteration =  902, loss = 6055179.32563\n",
      "iteration =  903, loss = 6068657.37925\n",
      "iteration =  904, loss = 6082150.41642\n",
      "iteration =  905, loss = 6095658.43713\n",
      "iteration =  906, loss = 6109181.44140\n",
      "iteration =  907, loss = 6122719.42922\n",
      "iteration =  908, loss = 6136272.40059\n",
      "iteration =  909, loss = 6149840.35551\n",
      "iteration =  910, loss = 6163423.29398\n",
      "iteration =  911, loss = 6177021.21600\n",
      "iteration =  912, loss = 6190634.12158\n",
      "iteration =  913, loss = 6204262.01070\n",
      "iteration =  914, loss = 6217904.88337\n",
      "iteration =  915, loss = 6231562.73960\n",
      "iteration =  916, loss = 6245235.57937\n",
      "iteration =  917, loss = 6258923.40270\n",
      "iteration =  918, loss = 6272626.20957\n",
      "iteration =  919, loss = 6286344.00000\n",
      "iteration =  920, loss = 6300076.77398\n",
      "iteration =  921, loss = 6313824.53151\n",
      "iteration =  922, loss = 6327587.27259\n",
      "iteration =  923, loss = 6341364.99721\n",
      "iteration =  924, loss = 6355157.70539\n",
      "iteration =  925, loss = 6368965.39713\n",
      "iteration =  926, loss = 6382788.07241\n",
      "iteration =  927, loss = 6396625.73124\n",
      "iteration =  928, loss = 6410478.37362\n",
      "iteration =  929, loss = 6424345.99956\n",
      "iteration =  930, loss = 6438228.60904\n",
      "iteration =  931, loss = 6452126.20207\n",
      "iteration =  932, loss = 6466038.77866\n",
      "iteration =  933, loss = 6479966.33879\n",
      "iteration =  934, loss = 6493908.88248\n",
      "iteration =  935, loss = 6507866.40972\n",
      "iteration =  936, loss = 6521838.92051\n",
      "iteration =  937, loss = 6535826.41485\n",
      "iteration =  938, loss = 6549828.89273\n",
      "iteration =  939, loss = 6563846.35417\n",
      "iteration =  940, loss = 6577878.79916\n",
      "iteration =  941, loss = 6591926.22771\n",
      "iteration =  942, loss = 6605988.63980\n",
      "iteration =  943, loss = 6620066.03544\n",
      "iteration =  944, loss = 6634158.41463\n",
      "iteration =  945, loss = 6648265.77738\n",
      "iteration =  946, loss = 6662388.12367\n",
      "iteration =  947, loss = 6676525.45352\n",
      "iteration =  948, loss = 6690677.76691\n",
      "iteration =  949, loss = 6704845.06386\n",
      "iteration =  950, loss = 6719027.34435\n",
      "iteration =  951, loss = 6733224.60840\n",
      "iteration =  952, loss = 6747436.85600\n",
      "iteration =  953, loss = 6761664.08715\n",
      "iteration =  954, loss = 6775906.30185\n",
      "iteration =  955, loss = 6790163.50010\n",
      "iteration =  956, loss = 6804435.68190\n",
      "iteration =  957, loss = 6818722.84725\n",
      "iteration =  958, loss = 6833024.99615\n",
      "iteration =  959, loss = 6847342.12861\n",
      "iteration =  960, loss = 6861674.24461\n",
      "iteration =  961, loss = 6876021.34416\n",
      "iteration =  962, loss = 6890383.42727\n",
      "iteration =  963, loss = 6904760.49392\n",
      "iteration =  964, loss = 6919152.54413\n",
      "iteration =  965, loss = 6933559.57789\n",
      "iteration =  966, loss = 6947981.59519\n",
      "iteration =  967, loss = 6962418.59605\n",
      "iteration =  968, loss = 6976870.58046\n",
      "iteration =  969, loss = 6991337.54842\n",
      "iteration =  970, loss = 7005819.49993\n",
      "iteration =  971, loss = 7020316.43499\n",
      "iteration =  972, loss = 7034828.35360\n",
      "iteration =  973, loss = 7049355.25576\n",
      "iteration =  974, loss = 7063897.14147\n",
      "iteration =  975, loss = 7078454.01074\n",
      "iteration =  976, loss = 7093025.86355\n",
      "iteration =  977, loss = 7107612.69991\n",
      "iteration =  978, loss = 7122214.51983\n",
      "iteration =  979, loss = 7136831.32330\n",
      "iteration =  980, loss = 7151463.11031\n",
      "iteration =  981, loss = 7166109.88088\n",
      "iteration =  982, loss = 7180771.63500\n",
      "iteration =  983, loss = 7195448.37266\n",
      "iteration =  984, loss = 7210140.09388\n",
      "iteration =  985, loss = 7224846.79865\n",
      "iteration =  986, loss = 7239568.48697\n",
      "iteration =  987, loss = 7254305.15884\n",
      "iteration =  988, loss = 7269056.81427\n",
      "iteration =  989, loss = 7283823.45324\n",
      "iteration =  990, loss = 7298605.07576\n",
      "iteration =  991, loss = 7313401.68183\n",
      "iteration =  992, loss = 7328213.27146\n",
      "iteration =  993, loss = 7343039.84463\n",
      "iteration =  994, loss = 7357881.40136\n",
      "iteration =  995, loss = 7372737.94163\n",
      "iteration =  996, loss = 7387609.46546\n",
      "iteration =  997, loss = 7402495.97284\n",
      "iteration =  998, loss = 7417397.46377\n",
      "iteration =  999, loss = 7432313.93824\n"
     ]
    }
   ],
   "source": [
    "num_iteration       = 1000\n",
    "learning_rate       = 0.01\n",
    "\n",
    "theta0              = 0\n",
    "theta1              = 0\n",
    "\n",
    "theta0_iteration    = np.zeros(num_iteration)\n",
    "theta1_iteration    = np.zeros(num_iteration)\n",
    "loss_iteration      = np.zeros(num_iteration)\n",
    "\n",
    "for i in range(num_iteration):\n",
    "\n",
    "    now_theta0 = theta0 \n",
    "    now_theta1 = theta1\n",
    "    \n",
    "    theta0  = theta0 - learning_rate * compute_gradient_theta0(x, y, now_theta0, now_theta1)\n",
    "    theta1  = theta1 - learning_rate * compute_gradient_theta1(x, y, now_theta0, now_theta1)\n",
    "    loss    = compute_loss(x, y, now_theta0, now_theta1)\n",
    "\n",
    "    theta0_iteration[i] = theta0\n",
    "    theta1_iteration[i] = theta1\n",
    "    loss_iteration[i]   = loss\n",
    "\n",
    "    print(\"iteration = %4d, loss = %5.5f\" % (i, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_regression(x, y, f):\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.title('linear regression result')\n",
    "\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curve(loss_iteration):\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.title('loss curve')\n",
    "\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_model_parameter(theta0_iteration, theta1_iteration):\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.title('model parameter')\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X0  = np.arange(-10, 10, 0.1)\n",
    "X1  = np.arange(-10, 10, 0.1)\n",
    "\n",
    "grid_theta0, grid_theta1 = \n",
    "\n",
    "grid_loss   = \n",
    "\n",
    "\n",
    "def plot_loss_surface(grid_theta0, grid_theta1, grid_loss):\n",
    "\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    plt.title('loss surface')\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 01. plot the input data in blue point and the regression result in red curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_regression(x, y, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 02. plot the values of the model parameters $\\theta_0$ in blue curve and $\\theta_1$ in green curve over the gradient descent iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_parameter(theta0_iteration, theta1_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 03. plot the loss values $\\mathcal{L}(\\theta)$ in red curve over the gradient descent iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curve(loss_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 04. plot the loss surface in 3-dimension surface where $x$-axis represents $\\theta_0$, $y$-axis represents $\\theta_1$ and $z$-axis represents $\\mathcal{L}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_surface(grid_theta0, grid_theta1, grid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
